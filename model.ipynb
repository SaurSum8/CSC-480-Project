{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f8bc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Image\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as f\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c5ff4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "ds = load_dataset(\"priyank-m/MJSynth_text_recognition\")\n",
    "\n",
    "labels = ds[\"train\"].unique(\"label\")\n",
    "label2id = {lab: i for i, lab in enumerate(labels)}\n",
    "\n",
    "def add_label_id(ex):\n",
    "    ex[\"label_id\"] = label2id[ex[\"label\"]]\n",
    "    return ex\n",
    "\n",
    "ds = ds.map(add_label_id)\n",
    "\n",
    "def preprocess(img):\n",
    "    w, h = img.size\n",
    "    new_w = int(w * (32 / h))\n",
    "    img = f.resize(img, (32, new_w))\n",
    "\n",
    "    if new_w < 256:\n",
    "        pad_right = 256 - new_w\n",
    "        img = f.pad(img, [0, 0, pad_right, 0], fill=0)\n",
    "    else:\n",
    "        img = f.crop(img, 0, 0, 32, 256) \n",
    "\n",
    "    return f.to_tensor(img)\n",
    "\n",
    "def process(batch):\n",
    "    batch[\"pixel_values\"] = [preprocess(im) for im in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "train_set = ds['train'].with_transform(process)\n",
    "test_set = ds['test'].with_transform(process)\n",
    "val_set = ds['val'].with_transform(process)\n",
    "\n",
    "def collate_fn(data):\n",
    "    x = torch.stack([d[\"pixel_values\"] for d in data])\n",
    "    y = torch.tensor([d[\"label_ids\"] for d in data], dtype=torch.long)\n",
    "    texts = [b[\"label\"] for b in data]\n",
    "    return {\"pixel_values\": x, \"label_ids\": y, \"labels\": texts}\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_set, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_set, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48155d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 7224600\n",
      "Test Size: 891924\n",
      "Val Size: 802733\n",
      "Total: 8919257\n",
      "\n",
      "Training Batches: 112885\n",
      "Test Batches: 13937\n",
      "Val Batches: 12543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Size: {len(train_set)}\\nTest Size: {len(test_set)}\\nVal Size: {len(val_set)}\\nTotal: {len(test_set) + len(train_set) + len(val_set)}\\n\")\n",
    "\n",
    "# batche size = 64\n",
    "print(f\"Training Batches: {len(train_dataloader)}\\nTest Batches: {len(test_dataloader)}\\nVal Batches: {len(val_dataloader)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e1bd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# shape = [3, 32, 256]\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8192, 4096)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(F.relu(self.conv1(x)))\n",
    "        out = self.pool2(F.relu(self.conv2(out)))\n",
    "        out = self.pool3(F.relu(self.conv3(out)))\n",
    "        out = self.pool4(F.relu(self.conv4(out)))\n",
    "\n",
    "        print(f\"Pre-flatten: {out.shape}\")\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        print(f\"Flatten: {out.shape}\")\n",
    "\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "num_classes = 23\n",
    "model = CNN(3, num_classes).to(device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc9107f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Pre-flatten: torch.Size([256, 2, 16])\n",
      "Flatten: torch.Size([256, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x32 and 8192x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m x = train_dataloader.dataset[i][\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     21\u001b[39m y = train_dataloader.dataset[i][\u001b[33m\"\u001b[39m\u001b[33mlabel_id\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m loss = loss_fn(logits, y)\n\u001b[32m     26\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CSC-480-Project/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CSC-480-Project/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     33\u001b[39m out = torch.flatten(out, start_dim=\u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFlatten: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m out = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m out = F.relu(\u001b[38;5;28mself\u001b[39m.fc2(out))\n\u001b[32m     38\u001b[39m out = F.relu(\u001b[38;5;28mself\u001b[39m.fc3(out))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CSC-480-Project/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CSC-480-Project/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CSC-480-Project/.venv/lib64/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (256x32 and 8192x4096)"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 15\n",
    "steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.perf_counter()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for i in range(len(train_dataloader)):\n",
    "        x = train_dataloader.dataset[i][\"pixel_values\"].to(device)\n",
    "        y = train_dataloader.dataset[i][\"label_id\"]\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean().item()\n",
    "        print(f\"Loss: {loss.item():.3f}, Acc: {acc:.3f}\")\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     val_loss = 0.0\n",
    "#     val_correct = 0\n",
    "#     val_total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_dataloader:\n",
    "#             x = batch[\"pixel_values\"].to(device)\n",
    "#             y = batch[\"label\"].to(device).long()\n",
    "\n",
    "#             logits = model(x)\n",
    "\n",
    "#             val_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "#             val_total += y.size(0)\n",
    "\n",
    "    elapsed_s = time.perf_counter() - epoch_start\n",
    "#     ms_per_step = (elapsed_s / steps_per_epoch) * 1000\n",
    "\n",
    "# model.eval()\n",
    "# test_correct = 0\n",
    "# test_total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_dataloader:\n",
    "#         x = batch[\"pixel_values\"].to(device)\n",
    "#         y = batch[\"label\"].to(device).long()\n",
    "\n",
    "#         outputs = model(x)\n",
    "#         test_correct += (outputs.argmax(dim=1) == y).sum().item()\n",
    "#         test_total += y.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51aadaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
